"/Users/afnan2/Desktop/ASA/Expermints/classifying_result_Social.csv",
row.names = T)
}
#calculate time of running function
systemTime = system.time(classifyDataSet(trainingData, testingData))
classifyDataSet <- function(trainingData, testingData) {
trainSize <- nrow(trainingData)
testSize <- nrow(testingData)
trainingMatrix <-
create_matrix(
trainingData["Tweet"],
removeNumbers = FALSE,
stemWords = FALSE,
weighting = weightTf,
minWordLength = 2,
removePunctuation = TRUE,
minDocFreq = 1,
stripWhitespace = TRUE
)
testingMatrix <-
create_matrix(
testingData["Tweet"],
removeNumbers = FALSE,
stemWords = FALSE,
weighting = weightTf,
minWordLength = 2,
removePunctuation = TRUE,
minDocFreq = 1,
stripWhitespace = TRUE
)
print("matrixes are created")
trainingContainer <-
create_container(
trainingMatrix,
trainingData$Polarity,
trainSize = 1:trainSize,
virgin = FALSE
)
testingContainer <-
create_container(
testingMatrix,
testingData$Polarity,
testSize = 1:testSize,
virgin = FALSE
)
print("containers are created")
#classifier  <- train_model(trainingContainer, "SVM", kernel = "linear")
classifier  <- train_model(trainingContainer, "MAXENT")
print("classifier is created")
#classifying_result_training <- classify_model(trainingContainer, classifier)
classifying_result <- classify_model(testingContainer, classifier)
print("classifying_result is created")
precision_recall <-
create_precisionRecallSummary(testingContainer, classifying_result)
print("precision_recall is created")
write.csv(precision_recall,
"/Users/afnan2/Desktop/ASA/Expermints/precision_recall_result_Social2.csv",
row.names = T)
write.csv(classifying_result,
"/Users/afnan2/Desktop/ASA/Expermints/classifying_result_Social2.csv",
row.names = T)
}
systemTime = system.time(classifyDataSet(trainingData, testingData))
trainingData = read.csv(
"/Users/afnan2/Dropbox/ASA Group/Dataset/Dataset by Domain/Political/Political_Training_80%_v2.csv"
)
testingData = read.csv("/Users/afnan2/Dropbox/ASA Group/Dataset/Dataset by Domain/Political/Political_Development_10%_v2.csv")
#rename columns name
colnames(trainingData) <- c("Tweet", "Polarity")
colnames(testingData) <- c("Tweet", "Polarity")
classifyDataSet <- function(trainingData, testingData) {
trainSize <- nrow(trainingData)
testSize <- nrow(testingData)
trainingMatrix <-
create_matrix(
trainingData["Tweet"],
removeNumbers = FALSE,
stemWords = FALSE,
weighting = weightTf,
minWordLength = 2,
removePunctuation = TRUE,
minDocFreq = 1,
stripWhitespace = TRUE
)
testingMatrix <-
create_matrix(
testingData["Tweet"],
removeNumbers = FALSE,
stemWords = FALSE,
weighting = weightTf,
minWordLength = 2,
removePunctuation = TRUE,
minDocFreq = 1,
stripWhitespace = TRUE
)
print("matrixes are created")
trainingContainer <-
create_container(
trainingMatrix,
trainingData$Polarity,
trainSize = 1:trainSize,
virgin = FALSE
)
testingContainer <-
create_container(
testSize = 1:testSize,
testingData$Polarity,
testingMatrix,
virgin = FALSE
)
print("containers are created")
#classifier  <- train_model(trainingContainer, "SVM", kernel = "linear")
classifier  <- train_model(trainingContainer, "MAXENT")
print("classifier is created")
#classifying_result_training <- classify_model(trainingContainer, classifier)
classifying_result <- classify_model(testingContainer, classifier)
print("classifying_result is created")
precision_recall <-
create_precisionRecallSummary(testingContainer, classifying_result)
print("precision_recall is created")
write.csv(precision_recall,
"/Users/afnan2/Desktop/ASA/Expermints/precision_recall_result_Political2.csv",
row.names = T)
write.csv(classifying_result,
"/Users/afnan2/Desktop/ASA/Expermints/classifying_result_Political2.csv",
row.names = T)
}
#calculate time of running function
systemTime = system.time(classifyDataSet(trainingData, testingData))
library(RTextTools)
library(tm)
library(e1071)
library(SparseM)
#setwd("/Users/afnan2/Desktop/ASA")
trainingData = read.csv(
"/Users/afnan2/Dropbox/ASA Group/Dataset/Dataset by Domain/Political/Political_Training_80%_v2.csv"
)
testingData = read.csv("/Users/afnan2/Dropbox/ASA Group/Dataset/Dataset by Domain/Political/Political_Development_10%_v2.csv")
#rename columns name
colnames(trainingData) <- c("Tweet", "Polarity")
colnames(testingData) <- c("Tweet", "Polarity")
classifyDataSet <- function(trainingData, testingData) {
trainSize <- nrow(trainingData)
testSize <- nrow(testingData)
trainingMatrix <-
create_matrix(
trainingData["Tweet"],
removeNumbers = FALSE,
stemWords = FALSE,
weighting = weightTf,
minWordLength = 2,
removePunctuation = TRUE,
minDocFreq = 1,
stripWhitespace = TRUE
)
testingMatrix <-
create_matrix(
testingData["Tweet"],
removeNumbers = FALSE,
stemWords = FALSE,
weighting = weightTf,
minWordLength = 2,
removePunctuation = TRUE,
minDocFreq = 1,
stripWhitespace = TRUE
)
print("matrixes are created")
trainingContainer <-
create_container(
trainingMatrix,
trainingData$Polarity,
trainSize = 1:trainSize,
virgin = FALSE
)
testingContainer <-
create_container(
testingMatrix,
testingData$Polarity,
testSize = 1:testSize,
virgin = FALSE
)
print("containers are created")
#classifier  <- train_model(trainingContainer, "SVM", kernel = "linear")
classifier  <- train_model(trainingContainer, "MAXENT")
print("classifier is created")
#classifying_result_training <- classify_model(trainingContainer, classifier)
classifying_result <- classify_model(testingContainer, classifier)
print("classifying_result is created")
precision_recall <-
create_precisionRecallSummary(testingContainer, classifying_result)
print("precision_recall is created")
write.csv(precision_recall,
"/Users/afnan2/Desktop/ASA/Expermints/precision_recall_result_Political2.csv",
row.names = T)
write.csv(classifying_result,
"/Users/afnan2/Desktop/ASA/Expermints/classifying_result_Political2.csv",
row.names = T)
}
#calculate time of running function
systemTime = system.time(classifyDataSet(trainingData, testingData))
trainingData = read.csv(
"/Users/afnan2/Dropbox/ASA Group/Dataset/Dataset by Domain/Sport/Sport_Training_80%.csv"
)
testingData = read.csv("/Users/afnan2/Dropbox/ASA Group/Dataset/Dataset by Domain/Sport/Sport_development_10%_2.csv")
#rename columns name
colnames(trainingData) <- c("Tweet", "Polarity")
colnames(testingData) <- c("Tweet", "Polarity")
classifyDataSet <- function(trainingData, testingData) {
trainSize <- nrow(trainingData)
testSize <- nrow(testingData)
trainingMatrix <-
create_matrix(
trainingData["Tweet"],
removeNumbers = FALSE,
stemWords = FALSE,
weighting = weightTf,
minWordLength = 2,
removePunctuation = TRUE,
minDocFreq = 1,
stripWhitespace = TRUE
)
testingMatrix <-
create_matrix(
testingData["Tweet"],
removeNumbers = FALSE,
stemWords = FALSE,
weighting = weightTf,
minWordLength = 2,
removePunctuation = TRUE,
minDocFreq = 1,
stripWhitespace = TRUE
)
print("matrixes are created")
trainingContainer <-
create_container(
trainingMatrix,
trainingData$Polarity,
trainSize = 1:trainSize,
virgin = FALSE
)
testingContainer <-
create_container(
testingMatrix,
testingData$Polarity,
testSize = 1:testSize,
virgin = FALSE
)
print("containers are created")
#classifier  <- train_model(trainingContainer, "SVM", kernel = "linear")
classifier  <- train_model(trainingContainer, "MAXENT")
print("classifier is created")
#classifying_result_training <- classify_model(trainingContainer, classifier)
classifying_result <- classify_model(testingContainer, classifier)
print("classifying_result is created")
precision_recall <-
create_precisionRecallSummary(testingContainer, classifying_result)
print("precision_recall is created")
write.csv(precision_recall,
"/Users/afnan2/Desktop/ASA/Expermints/precision_recall_result_Sport.csv",
row.names = T)
write.csv(classifying_result,
"/Users/afnan2/Desktop/ASA/Expermints/classifying_result_Sport.csv",
row.names = T)
}
#calculate time of running function
systemTime = system.time(classifyDataSet(trainingData, testingData))
trainingData = read.csv(
"/Users/afnan2/Dropbox/ASA Group/Dataset/Dataset by Domain/Tech/Tech_Training_80%.csv"
)
testingData = read.csv("/Users/afnan2/Dropbox/ASA Group/Dataset/Dataset by Domain/Tech/Tech_Development_10%.csv")
#rename columns name
colnames(trainingData) <- c("Tweet", "Polarity")
colnames(testingData) <- c("Tweet", "Polarity")
classifyDataSet <- function(trainingData, testingData) {
trainSize <- nrow(trainingData)
testSize <- nrow(testingData)
trainingMatrix <-
create_matrix(
trainingData["Tweet"],
removeNumbers = FALSE,
stemWords = FALSE,
weighting = weightTf,
minWordLength = 2,
removePunctuation = TRUE,
minDocFreq = 1,
stripWhitespace = TRUE
)
testingMatrix <-
create_matrix(
testingData["Tweet"],
removeNumbers = FALSE,
stemWords = FALSE,
weighting = weightTf,
minWordLength = 2,
removePunctuation = TRUE,
minDocFreq = 1,
stripWhitespace = TRUE
)
print("matrixes are created")
trainingContainer <-
create_container(
trainingMatrix,
trainingData$Polarity,
trainSize = 1:trainSize,
virgin = FALSE
)
testingContainer <-
create_container(
testingMatrix,
testingData$Polarity,
testSize = 1:testSize,
virgin = FALSE
)
print("containers are created")
#classifier  <- train_model(trainingContainer, "SVM", kernel = "linear")
classifier  <- train_model(trainingContainer, "MAXENT")
print("classifier is created")
#classifying_result_training <- classify_model(trainingContainer, classifier)
classifying_result <- classify_model(testingContainer, classifier)
print("classifying_result is created")
precision_recall <-
create_precisionRecallSummary(testingContainer, classifying_result)
print("precision_recall is created")
write.csv(precision_recall,
"/Users/afnan2/Desktop/ASA/Expermints/precision_recall_result_Tech.csv",
row.names = T)
write.csv(classifying_result,
"/Users/afnan2/Desktop/ASA/Expermints/classifying_result_Tech.csv",
row.names = T)
}
#calculate time of running function
systemTime = system.time(classifyDataSet(trainingData, testingData))
?searchTwitter()
library(twitteR)# for setup_twitter_oauth fun and searchTwitter fun
library(tm)
library(stringi)# for stri_trans_nfkc fun
library(stringr)# for str_replace_all fun
library(xlsx)
?searchTwitter
api_key <-"dWkY8fVFF42WEnpRMMCKiebm6"
api_secret<-"UIwytLJ78DEKmA2aSimY9xh3ekSyCurHFHQxPK437yAc2tRUUS"
access_token <- "4099695484-bnZ6yx1LEAZ12FIgFCsD1gvHNlhdcrldD5KyW6E"
access_token_secret <- "EbDjsJQ4G4iPA7AvKdpZR1EqJTDqBJ44WD8HaxFk1ftK8"
setup_twitter_oauth(api_key,api_secret,access_token,access_token_secret)
keyword=stri_trans_nfkc('#\u0627\u0644\u0645\u0644\u0643_\u064A\u0646\u062A\u0635\u0631_\u0644\u0642\u064A\u0627\u062F\u0629_\u0627\u0644\u0645\u0631\u0627\u0629')#œ«⁄‘, #unicode for Arabic word.
tweets = searchTwitter( keyowrd ,n=30000, lang='ar')# first parameter for "keyword", second parameter for number of tweets, third parameter for language.
tweets = searchTwitter( keyword ,n=30000, lang='ar')# first parameter for "keyword", second parameter for number of tweets, third parameter for language.
dataframe <- do.call("rbind", lapply(tweets, as.data.frame)); # convert list to dataframe
write.xlsx(dataframe,"/Users/afnan2/Desktop/Rawan/1_uncleaned.xlsx", row.names=F, sheetName = "Sheet1")
View(dataframe)
write.xlsx(dataframe,"/Users/afnan2/Desktop/Rawan/1_uncleaned.xlsx", row.names=F, sheetName = "Sheet1")
write.csv(dataframe, "/Users/afnan2/Desktop/Rawan/1_uncleaned.csv", row.names=F)
tweets = searchTwitter( '#SaudiWomen' ,n=20000, lang='ar')# first parameter for "keyword", second parameter for number of tweets, third parameter for language.
tweets = searchTwitter( '#SaudiWomen' ,n=20000)# first parameter for "keyword", second parameter for number of tweets, third parameter for language.
dataframe <- do.call("rbind", lapply(tweets, as.data.frame)); # convert list to dataframe
write.csv(dataframe, "/Users/afnan2/Desktop/Rawan/2_uncleaned.csv", row.names=F)
keyword=stri_trans_nfkc('#\u0627\u0644\u0634\u0639\u0628_\u0636\u062F_\u0642\u064A\u0627\u062F\u0629_\u0627\u0644\u0645\u0631\u0623\u0629')#œ«⁄‘, #unicode for Arabic word.
tweets = searchTwitter( keyword ,n=20000)# first parameter for "keyword", second parameter for number of tweets, third parameter for language.
dataframe <- do.call("rbind", lapply(tweets, as.data.frame)); # convert list to dataframe
write.csv(dataframe, "/Users/afnan2/Desktop/Rawan/3_uncleaned.csv", row.names=F)
keyword=stri_trans_nfkc('#\u0627\u0644\u0634\u0639\u0628_\u064A\u0631\u0641\u0636_\u0642\u064A\u0627\u062F\u0647_\u0627\u0644\u0645\u0631\u0627\u0647')#œ«⁄‘, #unicode for Arabic word.
tweets = searchTwitter( keyword ,n=20000)# first parameter for "keyword", second parameter for number of tweets, third parameter for language.
dataframe <- do.call("rbind", lapply(tweets, as.data.frame)); # convert list to dataframe
write.csv(dataframe, "/Users/afnan2/Desktop/Rawan/4_uncleaned.csv", row.names=F)
library(twitteR)# for setup_twitter_oauth fun and searchTwitter fun
library(tm)
library(stringi)# for stri_trans_nfkc fun
library(stringr)# for str_replace_all fun
library(xlsx)
#connection with twitter. (ASA__IU APP)
api_key <-"dWkY8fVFF42WEnpRMMCKiebm6"
api_secret<-"UIwytLJ78DEKmA2aSimY9xh3ekSyCurHFHQxPK437yAc2tRUUS"
access_token <- "4099695484-bnZ6yx1LEAZ12FIgFCsD1gvHNlhdcrldD5KyW6E"
access_token_secret <- "EbDjsJQ4G4iPA7AvKdpZR1EqJTDqBJ44WD8HaxFk1ftK8"
setup_twitter_oauth(api_key,api_secret,access_token,access_token_secret)
keyword=stri_trans_nfkc('#\u0642\u064A\u0627\u062F\u0629_\u0627\u0644\u0645\u0631\u0623\u0629')#œ«⁄‘, #unicode for Arabic word.
tweets = searchTwitter( keyword ,n=20000)# first parameter for "keyword", second parameter for number of tweets, third parameter for language.
dataframe <- do.call("rbind", lapply(tweets, as.data.frame)); # convert list to dataframe
write.csv(dataframe, "/Users/afnan2/Desktop/Rawan/5_uncleaned.csv", row.names=F)
keyword=stri_trans_nfkc('#\u0627\u0644\u0634\u0639\u0628_\u0636\u062F_\u0627\u0644\u0642\u064A\u0627\u062F\u0629')#œ«⁄‘, #unicode for Arabic word.
tweets = searchTwitter( keyword ,n=20000)# first parameter for "keyword", second parameter for number of tweets, third parameter for language.
api_key <-"N12zrFgZTbePnvqnVvNwv00E6"
api_secret<-"G0ybN5ebjZVQJwkMHaqawN9EUdMfltHCi8Hjw5YaHByJjGgBRH"
access_token <- "417173679-ffdicQpVAAteilpAJimlr8xHfIJzmVyABYzbP5yg"
access_token_secret <- "BWl46bE7uLYNE4R093lpQtG7cwROxK09eR0CrmcIK3nWD"
setup_twitter_oauth(api_key,api_secret,access_token,access_token_secret)
keyword=stri_trans_nfkc('#\u0627\u0644\u0634\u0639\u0628_\u0636\u062F_\u0627\u0644\u0642\u064A\u0627\u062F\u0629')#œ«⁄‘, #unicode for Arabic word.
tweets = searchTwitter( keyword ,n=20000)# first parameter for "keyword", second parameter for number of tweets, third parameter for language.
tweets = searchTwitter( keyword ,n=1000)# first parameter for "keyword", second parameter for number of tweets, third parameter for language.
keyword=stri_trans_nfkc('#\u0646\u062D\u0646_\u0628\u0646\u0627\u062A_\u0633\u0644\u0645\u0627\u0646_\u0644\u0627\u0646\u0631\u064A\u062F_\u0627\u0644\u0642\u064A\u0627\u062F\u0647')#œ«⁄‘, #unicode for Arabic word.
tweets = searchTwitter( keyword ,n=20000)# first parameter for "keyword", second parameter for number of tweets, third parameter for language.
dataframe <- do.call("rbind", lapply(tweets, as.data.frame)); # convert list to dataframe
write.csv(dataframe, "/Users/afnan2/Desktop/Rawan/7_uncleaned.csv", row.names=F)
keyword=stri_trans_nfkc('#\u0627\u0644\u0634\u0639\u0628_\u064A\u0648\u064A\u062F_\u0642\u064A\u0627\u062F\u0647_\u0627\u0644\u0645\u0631\u0627\u0647')#œ«⁄‘, #unicode for Arabic word.
tweets = searchTwitter( keyword ,n=20000)# first parameter for "keyword", second parameter for number of tweets, third parameter for language.
dataframe <- do.call("rbind", lapply(tweets, as.data.frame)); # convert list to dataframe
write.csv(dataframe, "/Users/afnan2/Desktop/Rawan/8_uncleaned.csv", row.names=F)
keyword=stri_trans_nfkc('#\u0627\u0644\u0633\u0645\u0627\u062D_\u0628\u0642\u064A\u0627\u062F\u0629_\u0627\u0644\u0645\u0631\u0623\u0629_\u0644\u0644\u0633\u064A\u0627\u0631\u0629')#œ«⁄‘, #unicode for Arabic word.
tweets = searchTwitter( keyword ,n=20000)# first parameter for "keyword", second parameter for number of tweets, third parameter for language.
dataframe <- do.call("rbind", lapply(tweets, as.data.frame)); # convert list to dataframe
write.csv(dataframe, "/Users/afnan2/Desktop/Rawan/9_uncleaned.csv", row.names=F)
tweets = searchTwitter( '#saudiwomencandrive' ,n=20000)# first parameter for "keyword", second parameter for number of tweets, third parameter for language.
dataframe <- do.call("rbind", lapply(tweets, as.data.frame)); # convert list to dataframe
write.csv(dataframe, "/Users/afnan2/Desktop/Rawan/10_uncleaned.csv", row.names=F)
library(RTextTools)
library(tm)
library(e1071)
setwd("/Users/afnan2/Dropbox/ASA Group/Lexicons/ASA lexicon/All_pos/")
file_list <- list.files()
dataset <- read.table(file, header=TRUE, sep=",")
for (file in file_list){
dataset <- read.table(file, header=TRUE, sep=",")
}
for (file in file_list){
# if the merged dataset doesn't exist, create it
if (!exists("dataset")){
dataset <- read.table(file, header=TRUE, sep=",")
}
# if the merged dataset does exist, append to it
if (exists("dataset")){
temp_dataset <-read.table(file, header=FALSE, sep=",")
dataset<-rbind(dataset, temp_dataset)
rm(temp_dataset)
}
}
setwd("/Users/afnan2/Dropbox/ASA Group/Lexicons/ASA lexicon/All_pos/")
file_list <- list.files()
file_list
for (file in file_list){
# if the merged dataset doesn't exist, create it
if (!exists("dataset")){
dataset <- read.table(file, header=TRUE, sep=",")
}
# if the merged dataset does exist, append to it
if (exists("dataset")){
temp_dataset <-read.table(file, header=FALSE, sep=",")
dataset<-rbind(dataset, temp_dataset)
rm(temp_dataset)
}
}
dataset = NULL
rm(temp_dataset)
for (file in file_list){
# if the merged dataset doesn't exist, create it
#if (!exists("dataset")){
# dataset <- read.table(file, header=TRUE, sep=",")
#}
# if the merged dataset does exist, append to it
if (exists("dataset")){
temp_dataset <-read.table(file, header=FALSE, sep=",")
dataset<-rbind(dataset, temp_dataset)
rm(temp_dataset)
}
}
View(dataset)
Political = read.csv("Pos_all_Domain_BiWord.csv", header = T, stringsAsFactors = F)
Social = read.csv("Pos_all_Domain_UniWord.csv", header = T, stringsAsFactors = F)
nrow(Political) + nrow(Social)
write.csv(dataset, "Pos_AllDomains_Uniword&biword.csv", row.names=F)
setwd("/Users/afnan2/Dropbox/ASA Group/Lexicons/ASA lexicon/All_neg/")
Political = read.csv("Neg_all_Domain_BiWord.csv", header = T, stringsAsFactors = F)
Social = read.csv("Neg_all_Domain_UniWord.csv", header = T, stringsAsFactors = F)
nrow(Political) + nrow(Social)
file_list <- list.files()
file_list
dataset = NULL
for (file in file_list){
# if the merged dataset doesn't exist, create it
#if (!exists("dataset")){
# dataset <- read.table(file, header=TRUE, sep=",")
#}
# if the merged dataset does exist, append to it
if (exists("dataset")){
temp_dataset <-read.table(file, header=FALSE, sep=",")
dataset<-rbind(dataset, temp_dataset)
rm(temp_dataset)
}
}
write.csv(dataset, "Neg_AllDomains_Uniword&biword.csv", row.names=F)
setwd("/Users/afnan2/Dropbox/ASA Group/Lexicons/ASA lexicon/Old Files/Lexicon by Domain/Political/")
file_list <- list.files()
file_list
dataset = NULL
for (file in file_list){
# if the merged dataset doesn't exist, create it
#if (!exists("dataset")){
# dataset <- read.table(file, header=TRUE, sep=",")
#}
# if the merged dataset does exist, append to it
if (exists("dataset")){
temp_dataset <-read.table(file, header=FALSE, sep=",")
dataset<-rbind(dataset, temp_dataset)
rm(temp_dataset)
}
}
setwd("/Users/afnan2/Dropbox/ASA Group/Lexicons/ASA lexicon/Political/")
file_list <- list.files()
file_list
dataset = NULL
for (file in file_list){
# if the merged dataset doesn't exist, create it
#if (!exists("dataset")){
# dataset <- read.table(file, header=TRUE, sep=",")
#}
# if the merged dataset does exist, append to it
if (exists("dataset")){
temp_dataset <-read.table(file, header=FALSE, sep=",")
dataset<-rbind(dataset, temp_dataset)
rm(temp_dataset)
}
}
write.csv(dataset, "Pos_Political_Uniword&biword.csv", row.names=F)
